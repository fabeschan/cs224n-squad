We have done a brief exploration of the data, such as collecting the sizes and dimensions of the training set. Our results showed that, there are 81381 training examples in total. We have also calculated the largest size of each input (both question and context). The maximum length of question is 60 and the maximum length of context is 766.

We decided to use Keras to build our model. The reason is that it should hopefully make it easier and quicker to build and try out new models compared to using Tensorflow. We installed nvidia-docker and was able to successfully test a very simple feed-forward NN model on the GPU via nvidia-docker. For the training step we encode the answer as a binary vector with length equal to the context vector where if the word at that index is part of the answer than the entry is 1, and 0 otherwise. Currently our model only cares about accuracy and not F1 which is something to keep in mind. We find that the training accuracy score rises from 10% to 40% but validation accuracy remains extremely low, which might mean we are overfitting or something else. This will require some investigation. Next we will work on extracting the hidden layer out from a decoder-encoder setup using Keras.
